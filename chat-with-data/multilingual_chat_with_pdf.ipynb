{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joReEVmEQKK5"
      },
      "source": [
        "# 📚 Multilingual Chat with PDF (Powered by SUTRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMvVqesEZ9pY"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://play-lh.googleusercontent.com/_O9p4Z4yucA2NLmZBu9mTJCuBwXeT9NcbtrDN6I8gKlkIPRySV0adOmbyipjSj9Gew\" width=\"130\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>SUTRA by TWO Platforms</h2>\n",
        "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRA’s dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
        "\n",
        "</div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/18BffQ6e0xrIpF97jRniLci2mAEvxHSl7?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGL1As_GaIt-"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uQQHhIiN3CH"
      },
      "source": [
        "### 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nPoxsbOLSoQ",
        "outputId": "d19adb48-79c6-4ed2-ac42-eadd182a0162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_openai langchain-community faiss-cpu requests pypdf python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6HKziXMN6-2"
      },
      "source": [
        "### STEP 2 : Setup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcNUI6fWLZ0i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqLPi81OBVR"
      },
      "source": [
        "### STEP 3 :  Load the PDF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpZ58RjuLfeC",
        "outputId": "fdeecbce-263a-4e16-f5c7-81e22bafc45b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 15 pages.\n"
          ]
        }
      ],
      "source": [
        "# 📍 STEP 3: Load the PDF Document (replace 'example.pdf' as needed)\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} pages.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdiY8_hpOPjp"
      },
      "source": [
        "### STEP 4 :  Split Documents into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xRJTvRSLx6U",
        "outputId": "5cbd7c99-7758-4f84-f53c-86021b29f71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 49 chunks.\n"
          ]
        }
      ],
      "source": [
        "# 📍 STEP 4: Split Documents into Chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGZNvievOVz4"
      },
      "source": [
        "###STEP 5 :  Create Embeddings + FAISS Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShVnNGD1L1Nf"
      },
      "outputs": [],
      "source": [
        "# 📍 STEP 5: Create Embeddings + FAISS Vector Store\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGDRWRNGOaMT"
      },
      "source": [
        "###STEP 6 :  Set Up Conversation Memory and RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYFEFqY4MOYy",
        "outputId": "35eec509-c9d5-4140-a551-eb6805f8ba60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-fc5334be7585>:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "# 📍 STEP 6: Set Up Conversation Memory and RAG Chain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# RAG model using Sutra\n",
        "rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatOpenAI(\n",
        "        api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "        base_url=\"https://api.two.ai/v2\",\n",
        "        model=\"sutra-v2\",\n",
        "        temperature=0.5\n",
        "    ),\n",
        "    retriever=retriever,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PXjWo34Og9W"
      },
      "source": [
        "###STEP 7 :  Ask Questions (Supports Multiple Languages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eo8C8HTMUzI"
      },
      "outputs": [],
      "source": [
        "# 📍 STEP 7: Ask Questions (Supports Multiple Languages)\n",
        "def ask_question(question, language=\"English\"):\n",
        "    rag_response = rag_chain.invoke(question)\n",
        "    context = rag_response[\"answer\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant that answers questions about documents.\n",
        "    Use the following context to answer the question:\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    Please respond in {language}.\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    chat = ChatOpenAI(\n",
        "        api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "        base_url=\"https://api.two.ai/v2\",\n",
        "        model=\"sutra-v2\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    from langchain.schema import HumanMessage\n",
        "    response = chat.invoke([HumanMessage(content=prompt)])\n",
        "    return response.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dm4JfHOOo79"
      },
      "source": [
        "###STEP 8: Try It Out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASF6fpBUMakG",
        "outputId": "f26ff0e0-54e0-4dc5-b06c-6e330b9e29f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹 Response:\n",
            " Transformer एक न्यूरल नेटवर्क आर्किटेक्चर है जो पूरी तरह से ध्यान तंत्र (attention mechanisms) पर निर्भर करता है, जिससे पुनरावृत्त (recurrent) या संकुचन (convolutional) परतों की आवश्यकता समाप्त हो जाती है। इसे 2017 में वासवानी और अन्य द्वारा लिखी गई पत्र \"Attention Is All You Need\" में पेश किया गया था। \n",
            "\n",
            "Transformer मॉडल एक एनकोडर-डीकोडर संरचना में होता है, जहाँ:\n",
            "\n",
            "- **एनकोडर** इनपुट अनुक्रम को प्रोसेस करता है और निरंतर प्रतिनिधित्व (continuous representations) उत्पन्न करता है।\n",
            "- **डीकोडर** इन प्रतिनिधियों को लेता है और आउटपुट अनुक्रम को एक समय में एक तत्व के रूप में उत्पन्न करता है, पहले से उत्पन्न प्रतीकों का उपयोग करते हुए।\n",
            "\n",
            "Transformer की प्रमुख विशेषताएँ हैं:\n",
            "\n",
            "1. **सेल्फ-अटेंशन तंत्र**: यह मॉडल को वाक्य में विभिन्न शब्दों के महत्व को एक दूसरे के सापेक्ष मापने की अनुमति देता है, चाहे वे अनुक्रम में कितनी भी दूर क्यों न हों। यह शब्दों के बीच संबंधों और निर्भरताओं को पकड़ने के लिए महत्वपूर्ण है।\n",
            "\n",
            "2. **मल्टी-हेडेड अटेंशन**: कई ध्यान तंत्र समानांतर में चलते हैं, जिससे मॉडल इनपुट के विभिन्न भागों पर एक साथ ध्यान केंद्रित कर सकता है।\n",
            "\n",
            "3. **पैरेललाइजेशन**: पुनरावृत्त मॉडलों के विपरीत, जो अनुक्रमों को क्रमिक रूप से प्रोसेस करते हैं, Transformers पूरे अनुक्रमों को एक साथ प्रोसेस कर सकते हैं, जिससे प्रशिक्षण की गति और दक्षता में महत्वपूर्ण सुधार होता है।\n",
            "\n",
            "4. **राज्य-के-राज्य प्रदर्शन**: Transformer ने विभिन्न प्राकृतिक भाषा प्रसंस्करण कार्यों में राज्य-के-राज्य परिणाम प्राप्त किए हैं, जैसे मशीन अनुवाद, जिससे यह पुनरावृत्त या संकुचन आर्किटेक्चर पर आधारित पिछले मॉडलों को पीछे छोड़ देता है।\n",
            "\n",
            "कुल मिलाकर, Transformer आर्किटेक्चर आधुनिक गहरी सीखने के अनुप्रयोगों में एक मौलिक आधार बन गया है।\n"
          ]
        }
      ],
      "source": [
        "# 📍 STEP 8: Try It Out!\n",
        "response = ask_question(\"what is transformer\", language=\"Hindi\")\n",
        "print(\"🔹 Response:\\n\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kFNYjZeOujB"
      },
      "source": [
        "##Finally Integrated UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "f1ff83b70fdd49a4916cb7b296c0e78e",
            "936845a96593421797289fa20733a6a7",
            "28691eacbf0e4b24bb8a7996cd0faa51",
            "a9c8ed7564854afdbb1cb917bd31f9a1",
            "57a2dc693edc439a9cc21b407f8a8122",
            "778b4cc3bfb740d6adf544ff071aba17",
            "ee6b3b899e3d4e6f8def68470abc4583",
            "d84d384ccb5a46148f2ecd3a610c54a4",
            "d030517a5ec94d93a094acd8dd663e26",
            "717d1e582b15493c8b96f48e5b2e798a",
            "a3ae2f9e741444149a0d94d098341638",
            "628aff5227c8414aba2fef292f785a91",
            "abd9f2d613d34e24a385a79f25de8608",
            "d338366d8471451981d3891823f2f6d5",
            "1c312031cd37450884e7f59c028989af",
            "6b7d5493d192465ba201aa5e45cb5226",
            "ef4243380ef24f61b6294dd4a4ff2c53",
            "8250627b7bc3453ab8b88ac29e484ae2",
            "b3f275d0f8ef42f0978a0cffe5ccc40c",
            "623a3a1edde4499ba81ae9da3c69d39d",
            "532ab4b2b3904ac7988dbf074d834eeb",
            "0c8fab512c814a26b46c5df397706931",
            "7f4b259817524dd3a415cffcea794ee8",
            "2051a7efb9c34baea9e9fcca9e27399c",
            "77cb84bba13e45359772e5702ee6504a",
            "92f5f5e71bfb4fca9d5a762ac6f80a69",
            "02c6a8b7868b48a3984640492cb24d3b",
            "6a34d03adda94e2c9df6439aec967360",
            "b8c6a87ed173496a81b43ed1380132ee"
          ]
        },
        "id": "v7ENw2ZJM9iV",
        "outputId": "be9243e6-3193-4f4a-f9b9-0aea0245fffa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1ff83b70fdd49a4916cb7b296c0e78e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value=\"<h3 style='font-family:Arial;'>📚 Multilingual Chat with PDF (Upload from Local)</h3…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1. Imports\n",
        "import os\n",
        "import requests\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.schema import HumanMessage\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "# 2. Setup LLM (Sutra)\n",
        "def get_sutra_model():\n",
        "    return ChatOpenAI(\n",
        "        api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "        base_url=\"https://api.two.ai/v2\",\n",
        "        model=\"sutra-v2\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "# 3. Load and index PDF\n",
        "def load_and_index_pdf(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=get_sutra_model(),\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "# 4. UI Components\n",
        "\n",
        "# PDF File Upload widget\n",
        "pdf_file_upload = widgets.FileUpload(\n",
        "    accept='.pdf',\n",
        "    multiple=False,\n",
        "    description='📁 Upload PDF',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "load_pdf_button = widgets.Button(\n",
        "    description=\"🔄 Load PDF\",\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='150px')\n",
        ")\n",
        "\n",
        "status_output = widgets.Output()\n",
        "\n",
        "# Language dropdown\n",
        "languages = [\n",
        "    \"English\", \"Hindi\", \"Gujarati\", \"Bengali\", \"Tamil\",\n",
        "    \"Telugu\", \"Kannada\", \"Malayalam\", \"Punjabi\", \"Marathi\",\n",
        "    \"Urdu\", \"Assamese\", \"Odia\", \"Sanskrit\", \"Korean\",\n",
        "    \"Japanese\", \"Arabic\", \"French\", \"German\", \"Spanish\",\n",
        "    \"Portuguese\", \"Russian\", \"Chinese\", \"Vietnamese\", \"Thai\",\n",
        "    \"Indonesian\", \"Turkish\", \"Polish\", \"Ukrainian\", \"Dutch\",\n",
        "    \"Italian\", \"Greek\", \"Hebrew\", \"Persian\", \"Swedish\",\n",
        "    \"Norwegian\", \"Danish\", \"Finnish\", \"Czech\", \"Hungarian\",\n",
        "    \"Romanian\", \"Bulgarian\", \"Croatian\", \"Serbian\", \"Slovak\",\n",
        "    \"Slovenian\", \"Estonian\", \"Latvian\", \"Lithuanian\", \"Malay\",\n",
        "    \"Tagalog\", \"Swahili\"\n",
        "]\n",
        "\n",
        "lang_dropdown = widgets.Dropdown(\n",
        "    options=languages,\n",
        "    value=\"English\",\n",
        "    description='🌐 Language:',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "chat_output = widgets.HTML(\n",
        "    value=\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>Chat history will appear here...</div>\"\n",
        ")\n",
        "\n",
        "user_input = widgets.Text(\n",
        "    placeholder='Type your message...',\n",
        "    layout=widgets.Layout(flex='4', width='auto')\n",
        ")\n",
        "\n",
        "send_button = widgets.Button(\n",
        "    description=\"📤 Send\",\n",
        "    button_style='primary',\n",
        "    layout=widgets.Layout(flex='1', width='auto')\n",
        ")\n",
        "\n",
        "messages = []\n",
        "conversation_chain = None\n",
        "\n",
        "# 5. Load PDF Logic\n",
        "def on_load_pdf(b):\n",
        "    global conversation_chain\n",
        "    uploaded_files = pdf_file_upload.value\n",
        "\n",
        "    with status_output:\n",
        "        clear_output()\n",
        "        if not uploaded_files:\n",
        "            print(\"❌ Please upload a PDF file first.\")\n",
        "            return\n",
        "        try:\n",
        "            print(\"⏳ Processing uploaded PDF...\")\n",
        "\n",
        "            # Save uploaded content to a temp file\n",
        "            uploaded_file = list(uploaded_files.values())[0]\n",
        "            with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "                tmp.write(uploaded_file['content'])\n",
        "                tmp_path = tmp.name\n",
        "\n",
        "            conversation_chain = load_and_index_pdf(tmp_path)\n",
        "            print(\"✅ PDF loaded and indexed successfully!\")\n",
        "        except Exception as e:\n",
        "            print(\"❌ Error:\", e)\n",
        "\n",
        "load_pdf_button.on_click(on_load_pdf)\n",
        "\n",
        "# 6. Chat Interaction Logic\n",
        "def on_send_click(b):\n",
        "    global conversation_chain\n",
        "    if conversation_chain is None:\n",
        "        with status_output:\n",
        "            clear_output()\n",
        "            print(\"❌ Load a PDF first.\")\n",
        "        return\n",
        "\n",
        "    user_text = user_input.value.strip()\n",
        "    if not user_text:\n",
        "        return\n",
        "\n",
        "    lang = lang_dropdown.value\n",
        "    messages.append(f\"<b style='color:#13f22d;'>You:</b> {user_text}\")\n",
        "\n",
        "    context_response = conversation_chain.invoke(user_text)\n",
        "    rag_context = context_response['answer']\n",
        "\n",
        "    system_msg = f\"\"\"\n",
        "You are a helpful assistant answering based on a document.\n",
        "Use this context: {rag_context}\n",
        "Always reply in: {lang}\n",
        "Question: {user_text}\n",
        "\"\"\"\n",
        "\n",
        "    chat_model = get_sutra_model()\n",
        "    sutra_response = chat_model.invoke([HumanMessage(content=system_msg)])\n",
        "    assistant_reply = sutra_response.content.strip()\n",
        "\n",
        "    messages.append(f\"<b style='color:#007acc;'>Assistant ({lang}):</b> {assistant_reply}\")\n",
        "\n",
        "    chat_html = \"<br>\".join(messages)\n",
        "    chat_output.value = f\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>{chat_html}</div>\"\n",
        "\n",
        "    user_input.value = \"\"\n",
        "\n",
        "send_button.on_click(on_send_click)\n",
        "\n",
        "# 7. Final Layout\n",
        "input_row = widgets.HBox([user_input, send_button])\n",
        "pdf_row = widgets.HBox([pdf_file_upload, load_pdf_button])\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='font-family:Arial;'>📚 Multilingual Chat with PDF (Upload from Local)</h3>\"),\n",
        "    pdf_row,\n",
        "    lang_dropdown,\n",
        "    chat_output,\n",
        "    input_row,\n",
        "    status_output\n",
        "])\n",
        "\n",
        "# 8. Display the App\n",
        "display(ui)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
